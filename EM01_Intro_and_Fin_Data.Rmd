---
title: "Tutorial EM 1: Introduction & Financial Data"
output: 
  learnr::tutorial:
    progressive: true
    allow_skip: true
  html_document:
    toc: yes
    number_sections: yes
    toc_float:
      collapsed: no
  pdf_document:
    toc: yes
    number_sections: yes
runtime: shiny_prerendered
bibliography: book.bib  
---

```{r setup, message = FALSE, warning = FALSE}
library(tidyverse)
library(lubridate)
library(tidyquant)
library(RSQLite)
library(dbplyr)
library(scales)
db_path <- "/home/shared/data/tidy_finance.sqlite"
```


# Introduction to Tidy Finance

We start this lecture by downloading and working with the big data sets we learned about in Empirical Methods and familiarize ourselves with their use. All data sets have been downloaded and provided to you. Please (only) run all the necessary parts and familiarize yourself with everything. The major textbook for this is [Tidy Finance in R](https://www.tidy-finance.org/r/).

## Exercises

To familiarize yourselves with the basics which you should remember from statistics please read the first chapter of the book and do the following exercises:

1. Download daily prices for another stock market ticker of your choice from Yahoo!Finance with `tq_get()` from the `tidyquant` package. Plot two time series of the ticker's un-adjusted and adjusted closing prices. Explain the differences.
```{r ex1_1, exercise=TRUE, exercise.lines=10}

```
1. Compute daily net returns for the asset and visualize the distribution of daily returns in a histogram using 100 bins. Also, use `geom_vline()` to add a dashed red vertical line that indicates the 5 percent quantile of the daily returns. Compute summary statistics (mean, standard deviation, minimum and maximum) for the daily returns.
```{r ex1_2, exercise=TRUE, exercise.lines=10}

```
1. Take your code from before and generalize it such that you can perform all the computations for an arbitrary vector of tickers (e.g., `ticker <- c("AAPL", "MMM", "BA")`). Automate the download, the plot of the price time series, and create a table of return summary statistics for this arbitrary number of assets.
```{r ex1_3, exercise=TRUE, exercise.lines=10}

```
1. Are days with high aggregate trading volume often also days with large absolute returns? Find an appropriate visualization to analyze the question using the ticker `AAPL`.
```{r ex1_4, exercise=TRUE, exercise.lines=10}

```
1. Compute monthly returns from the downloaded stock market prices. Compute the vector of historical average returns and the sample variance-covariance matrix. Compute the minimum variance portfolio weights and the portfolio volatility and average returns. Visualize the mean-variance efficient frontier. Choose one of your assets and identify the portfolio which yields the same historical volatility but achieves the highest possible average return.
```{r ex1_5, exercise=TRUE, exercise.lines=10}

```
1. In the portfolio choice analysis, we restricted our sample to all assets trading every day since 2000. How is such a decision a problem when you want to infer future expected portfolio performance from the results?
```{r ex1_6, exercise=TRUE, exercise.lines=10}

```
1. The efficient frontier characterizes the portfolios with the highest expected return for different levels of risk. Identify the portfolio with the highest expected return per standard deviation. Which famous performance measure is close to the ratio of average returns to the standard deviation of returns?
```{r ex1_7, exercise=TRUE, exercise.lines=10}

```

# Accessing & Managing Financial Data

In a next step we use the database that I have already set up for you. All data sets that are downloaded in [Chapter 2 of Tidy Finance in R](https://www.tidy-finance.org/r/accessing-managing-financial-data.html) are available here.

## Setting Up a Database

There are many ways to set up and organize a database, depending on the use case. For our purpose, the most efficient way is to use an [SQLite](https://www.sqlite.org/index.html) database, which is the C-language library that implements a small, fast, self-contained, high-reliability, full-featured, SQL database engine. Note that [SQL](https://en.wikipedia.org/wiki/SQL) (Structured Query Language) is a standard language for accessing and manipulating databases and heavily inspired the `dplyr` functions. We refer to [this tutorial](https://www.w3schools.com/sql/sql_intro.asp) for more information on SQL.

There are two packages that make working with SQLite in R very simple: `RSQLite` embeds the SQLite database engine in R, and `dbplyr` is the database back-end for `dplyr`. These packages allow to set up a database to remotely store tables and use these remote database tables as if they are in-memory data frames by automatically converting `dplyr` into SQL. Check out the [`RSQLite`](https://cran.r-project.org/web/packages/RSQLite/vignettes/RSQLite.html) and [`dbplyr`](https://db.rstudio.com/databases/sqlite/) vignettes for more information.

```{r eval=FALSE}
library(RSQLite)
library(dbplyr)
```

An SQLite database is easily created (in our case loaded) - the code below is really all there is. You do not need any external software. Note that we use the `extended_types=TRUE` option to enable date types when storing and fetching data. Otherwise, date columns are stored and retrieved as integers.  We will use the resulting file `tidy_finance.sqlite` in the subfolder `data` for all subsequent chapters to retrieve our data. 

```{r}
tidy_finance <- dbConnect(
  SQLite(),
  db_path,
  extended_types = TRUE
)
```

Next, we could create a remote table with some Fama-French factor data by using the function `dbWriteTable()`, which copies this data to our SQLite-database.

```{r eval=FALSE}
  dbWriteTable(tidy_finance,
    "factors_ff_monthly",
    value = factors_ff_monthly,
    overwrite = TRUE
  )
```

We can use the remote table as an in-memory data frame by building a connection via `tbl()`.

```{r}
factors_ff_monthly_db <- tbl(tidy_finance, "factors_ff_monthly")
```

All `dplyr` calls are evaluated lazily, i.e., the data is not in our R session's memory, and the database does most of the work. You can see that by noticing that the output below does not show the number of rows. In fact, the following code chunk only fetches the top 10 rows from the database for printing.  

```{r}
factors_ff_monthly_db |>
  select(month, rf)
```

If we want to have the whole table in memory, we need to `collect()` it. You will see that we regularly load the data into the memory in the next chapters.

```{r}
factors_ff_monthly_db |>
  select(month, rf) |>
  collect()
```

Other data sets that are downloaded in chapter 2 of the book are described there and already available (read an familiarize yourselves with them)!

```{r}
factors_ff_daily_db <- tbl(tidy_finance, "factors_ff_daily")
industries_ff_monthly_db <- tbl(tidy_finance, "industries_ff_monthly")
factors_q_monthly_db <- tbl(tidy_finance, "factors_q_monthly")
macro_predictors_db <- tbl(tidy_finance, "macro_predictors")
cpi_monthly_db <- tbl(tidy_finance, "cpi_monthly")
crsp_monthly_db <- tbl(tidy_finance, "crsp_monthly")
compustat_db <- tbl(tidy_finance, "compustat")
```

From now on, all you need to do to access data that is stored in the database is to follow three steps: (i) Establish the connection to the SQLite database, (ii) call the table you want to extract, and (iii) collect the data. For your convenience, the following steps show all you need in a compact fashion.

```{r}
factors_q_monthly <- factors_q_monthly_db |> collect()
factors_q_monthly
```

## Managing SQLite Databases

Finally, at the end of our data chapter, we revisit the SQLite database itself. When you drop database objects such as tables or delete data from tables, the database file size remains unchanged because SQLite just marks the deleted objects as free and reserves their space for future uses. As a result, the database file always grows in size.

You might be interested in listing all the tables that are currently in your database. You can do this via the `dbListTables()` function. 

```{r}
dbListTables(tidy_finance)
```

This function comes in handy if you are unsure about the correct naming of the tables in your database. 

## Exercises

1. Download the monthly Fama-French factors manually from [Ken French's data library](https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html) and read them in via `read_csv()`. Validate that you get the same data as via the `frenchdata` package. 
```{r ex1_8, exercise=TRUE, exercise.lines=10}

```
1. Download the daily Fama-French 5 factors using the `frenchdata` package. Use `get_french_data_list()` to find the corresponding table name. After the successful download and conversion to the column format that we used above, compare the `rf`, `mkt_excess`, `smb`, and `hml` columns of `factors_ff3_daily` to `factors_ff5_daily`. Discuss any differences you might find. 
```{r ex1_9, exercise=TRUE, exercise.lines=10}

```
1. Retry the former job with the `ffdownload` package
```{r ex1_10, exercise=TRUE, exercise.lines=10}

```

# WRDS, CRSP, and Compustat

This chapter shows how to connect to [Wharton Research Data Services (WRDS)](https://wrds-www.wharton.upenn.edu/), a popular provider of financial and economic data for research applications. We use this connection to download the most commonly used data for stock and firm characteristics, CRSP and Compustat. Unfortunately, this data is not freely available, but most students and researchers typically have access to WRDS through their university libraries. Assuming that you have access to WRDS, we show you how to prepare and merge the databases and store them in the `SQLite`-database introduced in the previous chapter. We conclude this chapter by providing some tips for working with the WRDS database.

First, we load the packages that we use throughout this chapter. Later on, we load more packages in the sections where we need them. 
```{r, eval = TRUE}
library(tidyverse)
library(lubridate)
library(scales)
library(RSQLite)
library(dbplyr)
```

We use the same date range as in the previous chapter to ensure consistency.

```{r, eval = TRUE}
start_date <- ymd("1960-01-01")
end_date <- ymd("2021-12-31")
```

## Downloading and Preparing CRSP

[The Center for Research in Security Prices (CRSP)](https://crsp.org/) provides the most widely used data for US stocks. We use the `wrds` connection object that we just created to first access monthly CRSP return data. Actually, we need three tables to get the desired data: (i) the CRSP monthly security file,

```{r eval=FALSE}
msf_db <- tbl(tidy_finance, "msf")
msf <- msf_db %>% collect()
```

(ii) the identifying information,
```{r eval=FALSE}
msenames_db <- tbl(tidy_finance, "msenames")
msenames <- msenames_db %>% collect()
```

and (iii) the delisting information.
```{r eval=FALSE}
msedelist_db <- tbl(tidy_finance, "msedelist")
msedelist <- msedelist_db %>% collect()
```

We use the three remote tables to fetch the data we want to put into our local database. Just as above, the idea is that we let the WRDS database do all the work and just download the data that we actually need. We apply common filters and data selection criteria to narrow down our data of interest: (i) we keep only data in the time windows of interest, (ii) we keep only US-listed stocks as identified via share codes `shrcd` 10 and 11, and (iii) we keep only months within permno-specific start dates `namedt` and end dates `nameendt`. In addition, we add delisting codes  and returns. You can read up in the great textbook of @BaliEngleMurray2016 for an extensive discussion on the filters we apply in the code below.

```{r eval=FALSE}
crsp_monthly <- msf |>
  filter(date >= as.Date(start_date) & date <= as.Date(end_date)) |>
  inner_join(
    msenames |>
      filter(shrcd %in% c(10, 11)) |>
      select(permno, exchcd, siccd, namedt, nameendt),
    by = c("permno")
  ) |>
  filter(date >= namedt & date <= nameendt) |>
  mutate(month = paste0(substr(as.character(date), 1, 8),"01")) |>
  left_join(
    msedelist |>
      select(permno, dlstdt, dlret, dlstcd) |>
      mutate(month = paste0(substr(as.character(dlstdt), 1, 8),"01")),
    by = c("permno", "month")
  ) |>
  select(
    permno, # Security identifier
    date, # Date of the observation
    month, # Month of the observation
    ret, # Return
    shrout, # Shares outstanding (in thousands)
    altprc, # Last traded price in a month
    exchcd, # Exchange code
    siccd, # Industry code
    dlret, # Delisting return
    dlstcd # Delisting code
  ) |>
 # collect() |>
  mutate(
    month = ymd(month),
    shrout = shrout * 1000
  )
```

Now, we have all the relevant monthly return data in memory and proceed with preparing the data for future analyses. We perform the preparation step at the current stage since we want to avoid executing the same mutations every time we use the data in subsequent chapters. 

The first additional variable we create is market capitalization (`mktcap`), which is the product of the number of outstanding shares `shrout` and the last traded price in a month `altprc`. Note that in contrast to returns `ret`, these two variables are not adjusted ex-post for any corporate actions like stock splits. Moreover, the `altprc` is negative whenever the last traded price does not exist, and CRSP decides to report the mid-quote of the last available order book instead. Hence, we take the absolute value of the market cap. We also keep the market cap in millions of USD just for convenience as we do not want to print huge numbers in our figures and tables. In addition, we set zero market cap to missing as it makes conceptually little sense (i.e., the firm would be bankrupt).

```{r eval=FALSE}
crsp_monthly <- crsp_monthly |>
  mutate(
    mktcap = abs(shrout * altprc) / 10^6,
    mktcap = na_if(mktcap, 0)
  )
```

The next variable we frequently use is the one-month *lagged* market capitalization. Lagged market capitalization is typically used to compute value-weighted portfolio returns, as we demonstrate in a later chapter. The most simple and consistent way to add a column with lagged market cap values is to add one month to each observation and then join the information to our monthly CRSP data.

```{r eval=FALSE}
mktcap_lag <- crsp_monthly |>
  mutate(month = month %m+% months(1)) |>
  select(permno, month, mktcap_lag = mktcap)

crsp_monthly <- crsp_monthly |>
  left_join(mktcap_lag, by = c("permno", "month"))
```

If you wonder why we do not use the `lag()` function, e.g., via `crsp_monthly |> group_by(permno) |> mutate(mktcap_lag = lag(mktcap))`, take a look at the exercises.

Next, we follow @BaliEngleMurray2016 in transforming listing exchange codes to explicit exchange names.
```{r eval=FALSE}
crsp_monthly <- crsp_monthly |>
  mutate(exchange = case_when(
    exchcd %in% c(1, 31) ~ "NYSE",
    exchcd %in% c(2, 32) ~ "AMEX",
    exchcd %in% c(3, 33) ~ "NASDAQ",
    .default = "Other"
  ))
```

Similarly, we transform industry codes to industry descriptions following @BaliEngleMurray2016. Notice that there are also other categorizations of industries [e.g., @FamaFrench1997] that are commonly used.

```{r eval=FALSE}
crsp_monthly <- crsp_monthly |>
  mutate(industry = case_when(
    siccd >= 1 & siccd <= 999 ~ "Agriculture",
    siccd >= 1000 & siccd <= 1499 ~ "Mining",
    siccd >= 1500 & siccd <= 1799 ~ "Construction",
    siccd >= 2000 & siccd <= 3999 ~ "Manufacturing",
    siccd >= 4000 & siccd <= 4899 ~ "Transportation",
    siccd >= 4900 & siccd <= 4999 ~ "Utilities",
    siccd >= 5000 & siccd <= 5199 ~ "Wholesale",
    siccd >= 5200 & siccd <= 5999 ~ "Retail",
    siccd >= 6000 & siccd <= 6799 ~ "Finance",
    siccd >= 7000 & siccd <= 8999 ~ "Services",
    siccd >= 9000 & siccd <= 9999 ~ "Public",
    TRUE ~ "Missing"
  ))
```

We also construct returns adjusted for delistings as described by @BaliEngleMurray2016. The delisting of a security usually results when a company ceases operations, declares bankruptcy, merges, does not meet listing requirements, or seeks to become private. The adjustment tries to reflect the returns of investors who bought the stock in the month before the delisting and held it until the delisting date. After this transformation, we can drop the delisting returns and codes.

```{r eval=FALSE}
crsp_monthly <- crsp_monthly |>
  mutate(ret_adj = case_when(
    is.na(dlstcd) ~ ret,
    !is.na(dlstcd) & !is.na(dlret) ~ dlret,
    dlstcd %in% c(500, 520, 580, 584) |
      (dlstcd >= 551 & dlstcd <= 574) ~ -0.30,
    dlstcd == 100 ~ ret,
    TRUE ~ -1
  )) |>
  select(-c(dlret, dlstcd))
```

Next, we compute excess returns by subtracting the monthly risk-free rate provided by our Fama-French data. As we base all our analyses on the excess returns, we can drop adjusted returns and the risk-free rate from our tibble. Note that we ensure excess returns are bounded by -1 from below as a return less than -100% makes no sense conceptually. Before we can adjust the returns, we have to connect to our database and load the tibble `factors_ff_monthly`.

```{r eval=FALSE}
factors_ff_monthly <- tbl(tidy_finance, "factors_ff_monthly") |>
  collect()

crsp_monthly <- crsp_monthly |>
  left_join(factors_ff3_monthly,
    by = "month"
  ) |>
  mutate(
    ret_excess = ret_adj - rf,
    ret_excess = pmax(ret_excess, -1)
  ) |>
  select(-ret_adj, -rf)
```

Since excess returns and market capitalization are crucial for all our analyses, we can safely exclude all observations with missing returns or market capitalization. 

```{r eval=FALSE}
crsp_monthly <- crsp_monthly |>
  drop_na(ret_excess, mktcap, mktcap_lag)
```

Finally, we store the monthly CRSP file in our database (do not do it, it is already there). 

```{r eval=FALSE}
  dbWriteTable(tidy_finance,
    "crsp_monthly",
    value = crsp_monthly,
    overwrite = TRUE
  )
```

## First Glimpse of the CRSP Sample

Before we move on to other data sources, let us look at some descriptive statistics of the CRSP sample, which is our main source for stock returns. 

```{r}
crsp_monthly <- crsp_monthly_db %>% collect()
```

Figure 3.1 shows the monthly number of securities by listing exchange over time. NYSE has the longest history in the data, but NASDAQ lists a considerably large number of stocks. The number of stocks listed on AMEX decreased steadily over the last couple of decades. By the end of 2022, there were 2,778 stocks with a primary listing on NASDAQ, 1,358 on NYSE, 162 on AMEX, and only one belonged to the other category. 

```{r, fig211, fig.cap = "Number of stocks in the CRSP sample listed at each of the US exchanges.", fig.alt = "Title: Monthly number of securities by listing exchange. The figure shows a line chart with the number of securities by listing exchange from 1960 to 2021. In the earlier period, NYSE dominated as a listing exchange. There is a strong upwards trend for NASDAQ. Other listing exchanges do only play a minor role."}
crsp_monthly |>
  count(exchange, date) |>
  ggplot(aes(x = date, y = n, color = exchange, linetype = exchange)) +
  geom_line() +
  labs(
    x = NULL, y = NULL, color = NULL, linetype = NULL,
    title = "Monthly number of securities by listing exchange"
  ) +
  scale_x_date(date_breaks = "10 years", date_labels = "%Y") +
  scale_y_continuous(labels = comma)
```

Next, we look at the aggregate market capitalization grouped by the respective listing exchanges in Figure 3.2. To ensure that we look at meaningful data which is comparable over time, we adjust the nominal values for inflation. In fact, we can use the tables that are already in our database to calculate aggregate market caps by listing exchange and plotting it just as if they were in memory. All values in Figure 3.2 are at the end of `r year(end_date)` USD to ensure intertemporal comparability. NYSE-listed stocks have by far the largest market capitalization, followed by NASDAQ-listed stocks.

```{r, fig212, fig.cap = "Market capitalization is measured in billion USD, adjusted for consumer price index changes such that the values on the horizontal axis reflect the buying power of billion USD in December 2021.", fig.alt = "Title: Monthly market cap by listing exchange in billion USD as of Dec 2021. The figure shows a line chart of the total market capitalization of all stocks aggregated by the listing exchange from 1960 to 2021, with years on the horizontal axis and the corresponding market capitalization on the vertical axis. Historically, NYSE listed stocks had the highest market capitalization. In the more recent past, the valuation of NASDAQ listed stocks exceeded that of NYSE listed stocks."}
tbl(tidy_finance, "crsp_monthly") |>
  left_join(tbl(tidy_finance, "cpi_monthly"), by = "month") |>
  group_by(month, exchange) |>
  summarize(
    mktcap = sum(mktcap, na.rm = TRUE) / cpi,
    .groups = "drop"
  ) |>
  collect() |>
  mutate(month = ymd(month)) |>
  ggplot(aes(
    x = month, y = mktcap / 1000,
    color = exchange, linetype = exchange
  )) +
  geom_line() +
  labs(
    x = NULL, y = NULL, color = NULL, linetype = NULL,
    title = "Monthly market cap by listing exchange in billions of Dec 2022 USD"
  ) +
  scale_x_date(date_breaks = "10 years", date_labels = "%Y") +
  scale_y_continuous(labels = comma)
```

Of course, performing the computation in the database is not really meaningful because we can easily pull all the required data into our memory. The code chunk above is slower than performing the same steps on tables that are already in memory. However, we just want to illustrate that you can perform many things in the database before loading the data into your memory. Before we proceed, we load the monthly CPI data.

```{r}
cpi_monthly <- cpi_monthly_db |> collect()
```

Next, we look at the same descriptive statistics by industry. Figure 3.3 plots the number of stocks in the sample for each of the SIC industry classifiers. For most of the sample period, the largest share of stocks is in manufacturing, albeit the number peaked somewhere in the 90s. The number of firms associated with public administration seems to be the only category on the rise in recent years, even surpassing manufacturing at the end of our sample period.

```{r, fig213, fig.cap = "Number of stocks in the CRSP sample associated with different industries.", fig.alt = "Title: Monthly number of securities by industry. The figure shows a line chart of the number of securities by industry from 1960 to 2021 with years on the horizontal axis and the corresponding number on the vertical axis. Except for stocks that are assigned to the industry public administration, the number of listed stocks decreased steadily at least since 1996. As of 2021, the segment of firms within public administration is the largest in terms of the number of listed stocks."}
crsp_monthly_industry <- crsp_monthly |>
  left_join(cpi_monthly, by = "month") |>
  group_by(month, industry) |>
  summarize(
    securities = n_distinct(permno),
    mktcap = sum(mktcap) / mean(cpi),
    .groups = "drop"
  )

crsp_monthly_industry |>
  ggplot(aes(
    x = month,
    y = securities,
    color = industry,
    linetype = industry
  )) +
  geom_line() +
  labs(
    x = NULL, y = NULL, color = NULL, linetype = NULL,
    title = "Monthly number of securities by industry"
  ) +
  scale_x_date(date_breaks = "10 years", date_labels = "%Y") +
  scale_y_continuous(labels = comma)
```

We also compute the market cap of all stocks belonging to the respective industries and show the evolution over time in Figure 3.4. All values are again in terms of billions of end of 2021 USD. At all points in time, manufacturing firms comprise of the largest portion of market capitalization. Toward the end of the sample, however, financial firms and services begin to make up a substantial portion of the market cap.

```{r, fig214, fig.cap = "Market capitalization is measured in billion USD, adjusted for consumer price index changes such that the values on the y-axis reflect the buying power of billion USD in December 2021.", fig.alt = "Title: Monthly total market cap by industry in billions as of Dec 2021 USD. The figure shows a line chart of total market capitalization of all stocks in the CRSP sample aggregated by industry from 1960 to 2021 with years on the horizontal axis and the corresponding market capitalization on the vertical axis. Stocks in the manufacturing sector have always had the highest market valuation. The figure shows a general upwards trend during the most recent past. "}
crsp_monthly_industry |>
  ggplot(aes(
    x = month,
    y = mktcap / 1000,
    color = industry,
    linetype = industry
  )) +
  geom_line() +
  labs(
    x = NULL, y = NULL, color = NULL, linetype = NULL,
    title = "Monthly total market cap by industry in billions as of Dec 2022 USD"
  ) +
  scale_x_date(date_breaks = "10 years", date_labels = "%Y") +
  scale_y_continuous(labels = comma)
```

## Daily CRSP Data

*The daily file is not available to you in our database! The code is shown for academic purposes.*

Before we turn to accounting data, we provide a proposal for downloading daily CRSP data. While the monthly data from above typically fit into your memory and can be downloaded in a meaningful amount of time, this is usually not true for daily return data. The daily CRSP data file is substantially larger than monthly data and can exceed 20GB. This has two important implications: you cannot hold all the daily return data in your memory (hence it is not possible to copy the entire data set to your local database), and in our experience, the download usually crashes (or never stops) because it is too much data for the WRDS cloud to prepare and send to your R session. 

There is a solution to this challenge. As with many *big data* problems, you can split up the big task into several smaller tasks that are easy to handle. That is, instead of downloading data about many stocks all at once, download the data in small batches for each stock consecutively. Such operations can be implemented in `for()`-loops, where we download, prepare, and store the data for a single stock in each iteration. This operation might nonetheless take a couple of hours, so you have to be patient either way (we often run such code overnight). To keep track of the progress, you can use `txtProgressBar()`. Eventually, we end up with more than 68 million rows of daily return data. Note that we only store the identifying information that we actually need, namely `permno`, `date`, and `month` alongside the excess returns. We thus ensure that our local database contains only the data we actually use and that we can load the full daily data into our memory later. Notice that we also use the function `dbWriteTable()` here with the option to append the new data to an existing table, when we process the second and all following batches. 
```{r eval = FALSE}
dsf_db <- tbl(tidy_finance, "dsf")

factors_ff3_daily <- tbl(tidy_finance, "factors_ff3_daily") |>
  collect()

permnos <- tbl(tidy_finance, "crsp_monthly") |>
  distinct(permno) |>
  pull()

batch_size <- 100
batches <- ceiling(length(permnos) / batch_size)

for (j in 1:batches) {
  
  permno_batch <- permnos[
    ((j - 1) * batch_size + 1):min(j * batch_size, length(permnos))
  ]

  crsp_daily_sub <- dsf |> #dsf_db |>
    filter(permno %in% permno_batch &
      date >= start_date & date <= end_date) |>
    select(permno, date, ret) |>
    #collect() |>
    drop_na()

  if (nrow(crsp_daily_sub) > 0) {
    crsp_daily_sub <- crsp_daily_sub |>
      mutate(month = floor_date(date, "month")) |>
      left_join(factors_ff3_daily |>
        select(date, rf), by = "date") |>
      mutate(
        ret_excess = ret - rf,
        ret_excess = pmax(ret_excess, -1)
      ) |>
      select(permno, date, month, ret, ret_excess)

    dbWriteTable(tidy_finance,
      "crsp_daily",
      value = crsp_daily_sub,
      overwrite = ifelse(j == 1, TRUE, FALSE),
      append = ifelse(j != 1, TRUE, FALSE)
    )
  }

  cat("Batch", j, "out of", batches, "done (", 
      percent(j / batches), ")\n")
}
```

## Preparing Compustat data

Firm accounting data are an important source of information that we use in portfolio analyses in subsequent chapters. The commonly used source for firm financial information is Compustat provided by [S&P Global Market Intelligence,](https://www.spglobal.com/marketintelligence/en/) which is a global data vendor that provides financial, statistical, and market information on active and inactive companies throughout the world. For US and Canadian companies, annual history is available back to 1950 and quarterly as well as monthly histories date back to 1962.

To access Compustat data, we tap into our database, which hosts a copy of the `funda` table that contains annual firm-level information on North American companies.

```{r eval=FALSE}
funda_db <- tbl(tidy_finance, "funda")
funda <- funda_db %>% collect()
```

We follow the typical filter conventions and pull only data that we actually need: (i) we get only records in industrial data format, (ii) in the standard format (i.e., consolidated information in standard presentation), and (iii) only data in the desired time window.

```{r eval=FALSE}
compustat <- funda |>
  filter(
    indfmt == "INDL" &
      datafmt == "STD" &
      consol == "C" &
      datadate >= start_date & datadate <= end_date
  ) |>
  select(
    gvkey, # Firm identifier
    datadate, # Date of the accounting data
    seq, # Stockholders' equity
    ceq, # Total common/ordinary equity
    at, # Total assets
    lt, # Total liabilities
    txditc, # Deferred taxes and investment tax credit
    txdb, # Deferred taxes
    itcb, # Investment tax credit
    pstkrv, # Preferred stock redemption value
    pstkl, # Preferred stock liquidating value
    pstk, # Preferred stock par value
    capx, # Capital investment
    oancf, # Operating cash flow
    sale,  # Revenue
    cogs, # Costs of goods sold
    xint, # Interest expense
    xsga # Selling, general, and administrative expenses
  ) #|>
  #collect()
```

Next, we calculate the book value of preferred stock and equity inspired by the [variable definition in Ken French's data library.](https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/Data_Library/variable_definitions.html) Note that we set negative or zero equity to missing which is a common practice when working with book-to-market ratios [see @Fama1992 for details].

```{r eval=FALSE}
compustat <- compustat |>
  mutate(
    be = coalesce(seq, ceq + pstk, at - lt) +
      coalesce(txditc, txdb + itcb, 0) -
      coalesce(pstkrv, pstkl, pstk, 0),
    be = if_else(be <= 0, as.numeric(NA), be),
    op = (sale - coalesce(cogs, 0) - 
            coalesce(xsga, 0) - coalesce(xint, 0)) / be,
  )
```

We keep only the last available information for each firm-year group. Note that `datadate` defines the time the corresponding financial data refers to (e.g., annual report as of December 31, 2021). Therefore, `datadate` is not the date when data was made available to the public. Check out the exercises for more insights into the peculiarities of `datadate`.

```{r eval=FALSE}
compustat <- compustat |>
  mutate(year = year(datadate)) |>
  group_by(gvkey, year) |>
  filter(datadate == max(datadate)) |>
  ungroup()
```

We also compute the investment ratio `inv` according to Ken French's variable definitions as the change in total assets from one fiscal year to another. Note that we again use the approach using joins as introduced with the CRSP data above to construct lagged assets.

```{r eval=FALSE}
compustat <- compustat |> 
  left_join(
    compustat |> 
      select(gvkey, year, at_lag = at) |> 
      mutate(year = year + 1), by = c("gvkey", "year")
  ) |> 
  mutate(
    inv = at / at_lag - 1,
    inv = if_else(at_lag <= 0, as.numeric(NA), inv)
  )
```

With the last step, we are already done preparing the firm fundamentals. Thus, we can store them in our local database. 

```{r eval=FALSE}
  dbWriteTable(tidy_finance,
    "compustat",
    value = compustat,
    overwrite = TRUE
  )
```

## Merging CRSP with Compustat

Unfortunately, CRSP and Compustat use different keys to identify stocks and firms. CRSP uses `permno` for stocks, while Compustat uses `gvkey` to identify firms. Fortunately, a curated matching table on WRDS allows us to merge CRSP and Compustat, so we create a connection to the *CRSP-Compustat Merged* table (provided by CRSP).

```{r eval=FALSE}
ccmxpf_linktable_db <- tbl(tidy_finance, "ccmxpf_linktable")
ccmxpf_linktable <- ccmxpf_linktable_db %>% collect()
```

The linking table contains links between CRSP and Compustat identifiers from various approaches. However, we need to make sure that we keep only relevant and correct links, again following the description outlined in @BaliEngleMurray2016. Note also that currently active links have no end date, so we just enter the current date via `today()`.

```{r eval=FALSE}
ccmxpf_linktable <- ccmxpf_linktable |> #ccmxpf_linktable_db |>
  filter(linktype %in% c("LU", "LC") &
    linkprim %in% c("P", "C") &
    usedflag == 1) |>
  select(permno = lpermno, gvkey, linkdt, linkenddt) |>
  #collect() |>
  mutate(linkenddt = replace_na(linkenddt, today()))
```

We use these links to create a new table with a mapping between stock identifier, firm identifier, and month. We then add these links to the Compustat `gvkey` to our monthly stock data. 

```{r eval=FALSE}
ccm_links <- crsp_monthly |>
  inner_join(ccmxpf_linktable, 
             by = "permno", relationship = "many-to-many") |>
  filter(!is.na(gvkey) & 
           (date >= linkdt & date <= linkenddt)) |>
  select(permno, gvkey, date)

crsp_monthly <- crsp_monthly |>
  left_join(ccm_links, by = c("permno", "date"))
```

As the last step, we update the previously prepared monthly CRSP file with the linking information in our local database.

```{r eval=FALSE}
  dbWriteTable(tidy_finance,
    "crsp_monthly",
    value = crsp_monthly,
    overwrite = TRUE
  )
```

Before we close this chapter, let us look at an interesting descriptive statistic of our data. As the book value of equity plays a crucial role in many asset pricing applications, it is interesting to know for how many of our stocks this information is available. Hence, Figure 3.5 plots the share of securities with book equity values for each exchange. It turns out that the coverage is pretty bad for AMEX- and NYSE-listed stocks in the 60s but hovers around 80% for all periods thereafter. We can ignore the erratic coverage of securities that belong to the other category since there is only a handful of them anyway in our sample.

```{r}
compustat <- compustat_db %>% collect()
```

```{r, fig215, fig.cap = "End-of-year share of securities with book equity values by listing exchange.", fig.alt = "Title: Share of securities with book equity values by exchange. The figure shows a line chart of end-of-year shares of securities with book equity values by exchange from 1960 to 2021 with years on the horizontal axis and the corresponding share on the vertical axis. After an initial period with lower coverage in the early 1960s, typically, more than 80 percent of the entries in the CRSP sample have information about book equity values from Compustat."}
crsp_monthly |>
  group_by(permno, year = year(month)) |>
  filter(date == max(date)) |>
  ungroup() |>
  left_join(compustat, by = c("gvkey", "year")) |>
  group_by(exchange, year) |>
  summarize(
    share = n_distinct(permno[!is.na(be)]) / n_distinct(permno),
    .groups = "drop"
  ) |>
  ggplot(aes(
    x = year, 
    y = share, 
    color = exchange,
    linetype = exchange
    )) +
  geom_line() +
  labs(
    x = NULL, y = NULL, color = NULL, linetype = NULL,
    title = "Share of securities with book equity values by exchange"
  ) +
  scale_y_continuous(labels = percent) +
  coord_cartesian(ylim = c(0, 1))
```


## Exercises

1. Compute `mkt_cap_lag` using `lag(mktcap)` rather than using joins as above. Filter out all the rows where the lag-based market capitalization measure is different from the one we computed above. Why are the two measures they different?
```{r ex2_1, exercise=TRUE, exercise.lines=10}

```
1. Plot the average market capitalization of firms for each exchange and industry, respectively, over time. What do you find?
```{r ex2_2, exercise=TRUE, exercise.lines=10}

```
1. In the `compustat` table, `datadate` refers to the date to which the fiscal year of a corresponding firm refers to. Count the number of observations in Compustat by `month` of this date variable. What do you find? What does the finding suggest about pooling observations with the same fiscal year?
```{r ex2_3, exercise=TRUE, exercise.lines=10}

```
1.  Go back to the original Compustat data in `funda_db` and extract rows where the same firm has multiple rows for the same fiscal year. What is the reason for these observations?
```{r ex2_4, exercise=TRUE, exercise.lines=10}

```
1. Keep the last observation of `crsp_monthly` by year and join it with the `compustat` table. Create the following plots: (i) aggregate  book equity by exchange over time and (ii) aggregate annual book equity by industry over time. Do you notice any different patterns to the corresponding plots based on market capitalization?
```{r ex2_5, exercise=TRUE, exercise.lines=10}

```
1. Repeat the analysis of market capitalization for book equity, which we computed from the Compustat data. Then, use the matched sample to plot book equity against market capitalization. How are these two variables related?
```{r ex2_6, exercise=TRUE, exercise.lines=10}

```

# TRACE and FISD

In this chapter, we *would* dive into the US corporate bond market. Bond markets are far more diverse than stock markets, as most issuers have multiple bonds outstanding simultaneously with potentially very different indentures. This market segment is exciting due to its size (roughly 10 trillion USD outstanding), heterogeneity of issuers (as opposed to government bonds), market structure (mostly over-the-counter trades), and data availability. We introduce how to use bond characteristics from FISD and trade reports from TRACE and provide code to download and clean TRACE in R. 

*Unfortunately FISD is not available to us. If you are interested please read [Chapter 4](https://www.tidy-finance.org/r/trace-and-fisd.html) of Tidy Finance in R!*

# Other Data Providers

In the previous chapters, we introduced many ways to get financial data that researchers regularly use. We showed how to load data into R from Yahoo!Finance and commonly used file types, such as comma-separated or Excel files. Then, we introduced remotely connecting to WRDS and downloading data from there. However, this is only a subset of the vast amounts of data available these days.

In this short chapter, we aim to provide an overview of common alternative data providers for which direct access via R packages exists. Such a list requires constant adjustments because both data providers and access methods change. However, we want to emphasize two main insights: First, the number of R packages that provide access to (financial) data is large. Too large actually to survey here exhaustively. Instead, we can only cover the tip of the iceberg. Second, R provides the functionalities to access basically any form of files or data available online. Thus, even if a desired data source does not come with a well-established R package, chances are high that data can be retrieved by establishing your own API connection or by scrapping the content.

In our non-exhaustive list below, we restrict ourselves to listing data sources accessed through easy-to-use R packages. For further inspiration on potential data sources, we recommend reading the [R task view empirical finance.](https://cran.r-project.org/web/views/Finance.html) Further inspiration (on more general social sciences) can be found [here.](https://cengel.github.io/gearup2016/SULdataAccess.html)

| Source | Description  | R packages                                       |
|--------|---------------------------------------|-----------------|
|          | **Macroeconomic Variables**         |                         |
| FED                                                                       | The Federal Reserve Bank of St. Louis provides more than 818,000 US and international time series from 109 sources via the API FRED. The data is freely available and can be browsed online on the [FRED homepage.](https://fred.stlouisfed.org/)  \index{Data!FRED}                                                                                                                                                    | `fredr` [@fredr] and `alfred` [@alfred]                |
| ECB                                                                       | The European Central Bank's [Statistical Data Warehouse](https://sdw.ecb.europa.eu/) provides data on Euro area monetary policy, financial stability, and other topics relevant to the activities of the ECB and the European System of Central Banks (ESCB).                                                                                                                                          | `ecb` [@ecb]\index{Data!ECB}                                           |
|                                                                           | **Financial data**                                                                                                                                                                                                                                                                                                                                                                                     |                                                        |
| [Bloomberg](https://www.bloomberg.com/) \index{Data!Bloomberg}                                  | Bloomberg's Fundamental coverage includes current and normalized historical data for the balance sheet, income statement, cash flows statement, and financial ratios. Additionally, it provides industry-specific data for communications, consumer, energy, health care, and many more. In order to retrieve Bloomberg data, a paid subscription is needed.                                           | `Rblpapi` [@Rblpapi]                                   |
| [Refinitiv Eikon](https://www.refinitiv.com/en/financial-data) \index{Data!Eikon} | Eikon provides access to real-time market data, news, fundamental data, analytics, trading, and messaging tools. Refinitiv's Eikon is a paid service. Apart from the CRAN version, there is also `https://github.com/philaris/eikonapir`.                                                       | `DatastreamDSWS2R` [@DatastreamDSWS2R] and `eikonapir` |
| [Nasdaq Data Link (Quandl)](data.nasdaq.com/publishers/qdl)               | Quandl is a publisher of alternative data. Quandl publishes free data, scraped from many different sources from the web. However, some of the data requires specific subscriptions on the Quandl platform.                                                                                                                                                                                             | `Quandl` [@Quandl]                                     |
| Global factor data                                                        | The data repository of @Jensen2022b. They provide return data for characteristic-managed portfolios from around the world. The database includes factors for 153 characteristics in 13 themes, using data from 93 countries. Download the data [here.](https://jkpfactors.com/)                                                                                                                        |                                                        |
| Open Source Asset Pricing                                                 | The data repository of @Chen2022. They provide return data for over 200 trading strategies with different time periods and specifications. The authors also provide signals and explanations of the factor construction. Download the data [here.](https://www.openassetpricing.com/data)                                                                                                              |                                                        |
| [Simfin](https://simfin.com/)                                             | Simfin make fundamental financial data freely available to private investors, researchers, and students. The data provider applies automating data collection processes to collect a large set of publicly available information from firms' financial statements.                                                                                                                                     | `simfinapi` [@simfinapi]                               |
|                                                                           | **High-frequency data**                                                                                                                                                                                                                                                                                                                                                                                |                                                        |
| IEX                                                                       | The IEX Group operates the Investors Exchange (IEX), a stock exchange for US equities. IEX offers US reference and market data including end-of-day and *intraday pricing data*. IEX offers an API which is freely available.\index{High-frequency data}\index{Big data}                                                                                                                                                                         | `Riex` [@Riex]                                         |
| TAQ                                                                       | TAQ data provides subscribed users access to all trades and quotes for all issues traded on NYSE, Nasdaq, and the regional exchanges. [TAQ data](https://www.nyse.com/market-data/historical) can be accessed from WRDS via Postgres. The `highfrequency` package delivers useful workflows to clean TAQ data.\index{Data!TAQ}                                                                                         | `highfrequency` [@highfrequency]                       |
|                                                                           | **Other (free) data**                                                                                                                                                                                                                                                                                                                                                                                  |                                                        |
| [CoinMarketCap](https://coinmarketcap.com/)                                                               | The data provider CoinMarketCap provides cryptocurrency information and historical prices, as well as information on the exchanges they are listed on.\index{Cryptocurrency}                                                                                                                                                                                                                            | `crypto2` [@crypto2]                                   |
| [CoinGecko](https://www.coingecko.com/)                                                              | CoinGecko is an alternative crypto data provider of current and historical data on a myriad of coins and exchanges.                                                                                                                                                                                                                          | `geckor` [@geckor]                                   |
| Twitter\index{Twitter}                                                                   | Twitter provides (limited) access for academic research to extract and analyze Tweets.                                                                                                                                                                                                                                                                                                                 | `rtweet` [@rtweet]                                     |
| SEC company fillings                                                      | The [EDGAR](https://www.sec.gov/edgar/about) database provides free public access to corporate information, allowing you to research a public company's financial information and operations by reviewing the filings the company makes with the SEC. You can also research information provided by mutual funds (including money market funds), exchange-traded funds (ETFs), and variable annuities. | `edgarWebR` [@edgarWebR]                               |
| Google trends\index{Data!Google}                                                             | Google offers public access to global search volumes through its search engine through the [Google Trends portal.](https://trends.google.com/trends/?geo=DK)\index{Google trends}                                                                                                                                                                                                                                           | `globaltrends` [@globaltrends] and `gtrends` [@gtrendsR]                         |

## Exercises

1. Select one of the data sources in the table above and retrieve some data: Browse the homepage of the data provider or the package documentation to find inspiration on which type of data is available to you and how to download the data into your R session.
```{r ex4_1, exercise=TRUE, exercise.lines=10}

```
1. Generate summary statistics of the data you retrieved and provide some useful visualization. The possibilities are endless: Maybe there is some interesting economic event you want to analyze, such as stock market responses to Twitter activity.
```{r ex4_2, exercise=TRUE, exercise.lines=10}

```
1. [Simfin](https://simfin.com/) provides excellent data coverage. Use their API to find out if the information Simfin provides overlaps with the CRSP/Compustat dataset in the `tidy_finance.sqlite` database introduced in Chapters 2-4.
```{r ex4_3, exercise=TRUE, exercise.lines=10}

```

# References {-}

<div id="refs"></div>